\thispagestyle{empty}
\begin{center}
    A Study on Thermal Infrared Image Colorization Based on Semantic Information
\end{center}

\thispagestyle{empty}
\noindent ABSTRACT : Thermal infrared (TIR) cameras are cameras that capture thermal infrared radiation emitted from the surface of an object depending on its temperature. 
TIR cameras are used in different fields since they can capture clear images even in bad weather conditions like rain, fog, and sandstorms, and in low-light environments like nighttime and tunnels.
On the other hand, images captured by TIR cameras have the disadvantage that they do not have rich color information like visible light images, making the interpretation of situations more difficult than with visible light images. This is one of the factors that have hindered the widespread use of TIR cameras and also prevents the application of TIR images into computer vision algorithms for visible light images.

Hence, recent research has attempted to tackle the above problem by using neural networks to colorize TIR images and generate pseudo visible light color images.
This makes it possible to treat TIR images as visible light color images, allowing the user to benefit from both the robustness to situational changes inherent in TIR images and the ease of understanding due to the rich color information inherent in visible light color images.
However, images generated by existing methods often assimilate foreground objects with background areas, resulting in an unnatural appearance.

In this paper, we propose an image generation method that can colorize TIR images with higher quality than existing methods by combining a model for semantic segmentation with a generative adversarial network that can generate realistic images. 
Semantic segmentation is the task to divide an image into regions according to the object meaning.
Our method uses a semantic segmentation model to extract features that focus on the semantic information of objects in the input image, and uses these features as auxiliary inputs to reinforce the semantic understanding of the colorization model.
In addition, the local quality of the generated images is enhanced by using the generated label maps to extract regions belonging to specific classes, such as "car" and "person," and calculating the adversarial loss only for the corresponding regions.

The proposed method is a generative adversarial network that consists of a generator composed of two modules and two types of discriminators.
The generator consists of a colorization module, which receives the grayscale TIR image and generates the color image, and a segmentation module, which performs semantic segmentation on the input image.
%The generator consists of a colorization module that generates color images and a segmentation module that performs semantic segmentation on the input images.
The colorization module is a U-Net structured network that takes a grayscale TIR image and features extracted by the segmentation module, and generates a pseudo visible color image in RGB format.
The segmentation module is a pre-trained and weight-fixed semantic segmentation model that takes a TIR image and outputs a semantic label map of the same resolution.
In the process, features extracted from the input image and generated labels are passed to the colorization module to reinforce the semantic understanding of the input image and thus enhance the output quality.
To improve the local quality of the generated images, the class image discriminator is used in addition to the usual discriminator (global image discriminator).
The class image discriminator is applied to images with only one class extracted based on the semantic label map.
The loss function includes content loss computed from the L1 norm of the generated image and the Ground-Truth, adversarial loss computed from the global image discriminator, perceptual loss for reproducing high-level features, TV loss for 
\newpage
\thispagestyle{empty}
\noindent suppressing noise, and class adversarial loss computed from the class image discriminator.
We use RSGAN for two types of adversarial loss, which computes the loss based on the relative authenticity of the generated image and the Ground-Truth, aiming to stabilize the training and improve the quality of the generated image.

Through experiments on multiple datasets of TIR images, we have shown that the proposed method can produce images of equal or better quality than existing methods.
Although PSNR and SSIM of some datasets were lower than existing methods, the proposed method showed the best results in all datasets for LPIPS and FID, which are highly correlated with perceptual quality.
We also qualitatively confirmed that the proposed method reproduces regions of objects with natural colors and textures, where the existing method causes semantic confusion.
In addition, the results of a user study with multiple subjects showed the superiority of the proposed method.
We then performed ablation studies on the segmentation module and the class image discriminator, and showed that the two proposed components are effective in improving coloring quality. 
Various adversarial loss computation methods were applied to the proposed method, and the optimal loss computation method was discussed.
While the proposed method showed superior generation results, it failed to colorize some test images properly, which indicates the need for further improvement in data selection and input techniques.
Future work is needed to investigate a network structure that can keep the computational cost low while maintaining quality and processing method for input images that is optimal for colorization.